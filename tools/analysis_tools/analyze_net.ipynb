{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from mmcv.cnn import ConvModule, DepthwiseSeparableConvModule\n",
    "from mmcv.runner import BaseModule\n",
    "from mmdet.models.necks import YOLOXPAFPN\n",
    "from mmdet.models.backbones import Darknet, CSPDarknet,MobileNetV2\n",
    "\n",
    "from mmdet.models.dense_heads import YOLOXHead\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "from thop import profile,clever_format\n",
    "\n",
    "CLASSES = (\n",
    "        \"road_cone\", \"car_barrier\", \"ground_lock\", \"road_pile\", \n",
    "        \"warning_board\", \"shopping_cart\", \"ball_stone\", \"sharing_bike\",\n",
    "        \"electric_bike\", \"child\", \"others\",\n",
    "        )\n",
    "\n",
    "num_cls_rearview = len(CLASSES)\n",
    "\n",
    "img_scale = (480,640) # height, width\n",
    "\n",
    "\n",
    "class NaiveModel(nn.Module):\n",
    "    def __init__(self, backbone, neck, head):\n",
    "        super(NaiveModel, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.neck = neck\n",
    "        self.head = head\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.neck(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "img_inputs = torch.randn(1, 3, img_scale[0], img_scale[1])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# default Yolox Nano Config by ti\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 11, 60, 80])\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register count_upsample() for <class 'torch.nn.modules.upsampling.Upsample'>.\n",
      "model  #flops: 959.770M,\t#params: 898.704K\n"
     ]
    }
   ],
   "source": [
    "test_backbone = CSPDarknet(\n",
    "    deepen_factor=0.33,\n",
    "    widen_factor=0.25,\n",
    "    use_depthwise=True)\n",
    "test_neck = YOLOXPAFPN(\n",
    "    in_channels=[64,128,256],\n",
    "    out_channels=64,\n",
    "    num_csp_blocks=1,\n",
    "    use_depthwise=True,\n",
    "   )\n",
    "head = YOLOXHead(\n",
    "     num_classes=num_cls_rearview, \n",
    "        in_channels=64, \n",
    "        feat_channels=64, \n",
    "        use_depthwise=True,\n",
    ")\n",
    "m_ori = NaiveModel(\n",
    "        test_backbone,\n",
    "        test_neck,\n",
    "        head\n",
    ")\n",
    "outputs_ori = m_ori(img_inputs)\n",
    "print(outputs_ori[0][0].shape)\n",
    "\n",
    "ops, params = profile(m_ori, inputs=(img_inputs,))\n",
    "print('model  #flops: {},\\t#params: {}'.format(*clever_format([ops, params], \"%.3f\")))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# customized yolox lite Config by ti\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 11, 30, 40])\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register count_relu() for <class 'torch.nn.modules.activation.LeakyReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register count_upsample() for <class 'torch.nn.modules.upsampling.Upsample'>.\n",
      "model  #flops: 5.170G,\t#params: 2.527M\n"
     ]
    }
   ],
   "source": [
    "use_depthwise = False\n",
    "test_backbone = CSPDarknet(\n",
    "    arch='P4',\n",
    "    deepen_factor=0.5,\n",
    "    widen_factor=0.5,\n",
    "    out_indices=(3,),\n",
    "    use_depthwise=use_depthwise,\n",
    "    spp_kernal_sizes=(5, 9, 13),\n",
    "    act_cfg=dict(type='LeakyReLU', negative_slope=0.1))\n",
    "test_neck = YOLOXPAFPN(\n",
    "    in_channels=[256,],\n",
    "    out_channels=256,\n",
    "    num_csp_blocks=2,\n",
    "    use_depthwise=use_depthwise,\n",
    "    upsample_cfg=dict(scale_factor=2, mode='bilinear'),\n",
    "    conv_cfg=None,\n",
    "    norm_cfg=dict(type='BN', momentum=0.03, eps=0.001),\n",
    "    act_cfg=dict(type='LeakyReLU', negative_slope=0.1),\n",
    "   )\n",
    "head = YOLOXHead(\n",
    "    num_classes=num_cls_rearview,\n",
    "    in_channels=256,\n",
    "    feat_channels=128,\n",
    "    strides=[16],\n",
    "    use_depthwise=use_depthwise,\n",
    "    act_cfg=dict(type='LeakyReLU', negative_slope=0.1),\n",
    ")\n",
    "img_inputs = torch.randn(1, 3, img_scale[0], img_scale[1])\n",
    "\n",
    "\n",
    "m = NaiveModel(\n",
    "        test_backbone,\n",
    "        test_neck,\n",
    "        head\n",
    ")\n",
    "outputs = m(img_inputs)\n",
    "print(outputs[0][0].shape)\n",
    "# torch.onnx.export(m, img_inputs, \"litedbg.onnx\", verbose=True, opset_version=11)\n",
    "\n",
    "ops, params = profile(m, inputs=(img_inputs,))\n",
    "print('model  #flops: {},\\t#params: {}'.format(*clever_format([ops, params], \"%.3f\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edgeai_3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
